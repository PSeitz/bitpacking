
/** code generated by avxpacking-codegen.py starts here **/


/* we are going to pack 256 1-bit values, touching 1 256-bit words, using 16 bytes */ 
unsafe fn avxpackblock1(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  1 256-bit word */ 
  let mut w0: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 31));
  _mm256_storeu_si256(compressed.offset(0), w0);
}


/* we are going to pack 256 2-bit values, touching 2 256-bit words, using 32 bytes */ 
unsafe fn avxpackblock2(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  2 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 30));
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 30));
  _mm256_storeu_si256(compressed.offset(1), w1);
}


/* we are going to pack 256 3-bit values, touching 3 256-bit words, using 48 bytes */ 
unsafe fn avxpackblock3(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  3 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 27));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 1));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 28));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 29));
  _mm256_storeu_si256(compressed.offset(2), w0);
}


/* we are going to pack 256 4-bit values, touching 4 256-bit words, using 64 bytes */ 
unsafe fn avxpackblock4(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  4 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 28));
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 28));
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 28));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 28));
  _mm256_storeu_si256(compressed.offset(3), w1);
}


/* we are going to pack 256 5-bit values, touching 5 256-bit words, using 80 bytes */ 
unsafe fn avxpackblock5(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  5 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 25));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 23));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 26));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 31));
  w1 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 27));
  _mm256_storeu_si256(compressed.offset(4), w0);
}


/* we are going to pack 256 6-bit values, touching 6 256-bit words, using 96 bytes */ 
unsafe fn avxpackblock6(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  6 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 22));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 26));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 22));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 26));
  _mm256_storeu_si256(compressed.offset(5), w1);
}


/* we are going to pack 256 7-bit values, touching 7 256-bit words, using 112 bytes */ 
unsafe fn avxpackblock7(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  7 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 21));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 23));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 19));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 1));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 15));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 22));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 25));
  _mm256_storeu_si256(compressed.offset(6), w0);
}


/* we are going to pack 256 8-bit values, touching 8 256-bit words, using 128 bytes */ 
unsafe fn avxpackblock8(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  8 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 24));
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 24));
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 24));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 24));
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 24));
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 24));
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 24));
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 24));
  _mm256_storeu_si256(compressed.offset(7), w1);
}


/* we are going to pack 256 9-bit values, touching 9 256-bit words, using 144 bytes */ 
unsafe fn avxpackblock9(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  9 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 18));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 22));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 17));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 21));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 15));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 1));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 19));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 23));
  _mm256_storeu_si256(compressed.offset(8), w0);
}


/* we are going to pack 256 10-bit values, touching 10 256-bit words, using 160 bytes */ 
unsafe fn avxpackblock10(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  10 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 18));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 22));
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 18));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 22));
  _mm256_storeu_si256(compressed.offset(9), w1);
}


/* we are going to pack 256 11-bit values, touching 11 256-bit words, using 176 bytes */ 
unsafe fn avxpackblock11(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  11 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 11));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 1));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 13));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 25));
  w0 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 15));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 27));
  w0 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 17));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 18));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 19));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 21));
  _mm256_storeu_si256(compressed.offset(10), w0);
}


/* we are going to pack 256 12-bit values, touching 12 256-bit words, using 192 bytes */ 
unsafe fn avxpackblock12(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  12 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 20));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 20));
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 20));
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 20));
  _mm256_storeu_si256(compressed.offset(11), w1);
}


/* we are going to pack 256 13-bit values, touching 13 256-bit words, using 208 bytes */ 
unsafe fn avxpackblock13(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  13 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 13));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 21));
  w0 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 15));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 9));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 17));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 11));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 18));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 31));
  w1 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 25));
  w0 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 19));
  _mm256_storeu_si256(compressed.offset(12), w0);
}


/* we are going to pack 256 14-bit values, touching 14 256-bit words, using 224 bytes */ 
unsafe fn avxpackblock14(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  14 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 18));
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 18));
  _mm256_storeu_si256(compressed.offset(13), w1);
}


/* we are going to pack 256 15-bit values, touching 15 256-bit words, using 240 bytes */ 
unsafe fn avxpackblock15(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  15 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 15));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 13));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 11));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 9));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 1));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(16)) , 16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 27));
  w0 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 19));
  w0 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(30)) , 2));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 17));
  _mm256_storeu_si256(compressed.offset(14), w0);
}


/* we are going to pack 256 16-bit values, touching 16 256-bit words, using 256 bytes */ 
unsafe fn avxpackblock16(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  16 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(1)) , 16));
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 16));
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 16));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 16));
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 16));
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 16));
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 16));
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 16));
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 16));
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 16));
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 16));
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 16));
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 16));
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 16));
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 16));
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 16));
  _mm256_storeu_si256(compressed.offset(15), w1);
}


/* we are going to pack 256 17-bit values, touching 17 256-bit words, using 272 bytes */ 
unsafe fn avxpackblock17(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  17 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 19));
  w0 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 27));
  w0 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 14));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 9));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 11));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 13));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 15));
  _mm256_storeu_si256(compressed.offset(16), w0);
}


/* we are going to pack 256 18-bit values, touching 18 256-bit words, using 288 bytes */ 
unsafe fn avxpackblock18(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  18 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 14));
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 14));
  _mm256_storeu_si256(compressed.offset(17), w1);
}


/* we are going to pack 256 19-bit values, touching 19 256-bit words, using 304 bytes */ 
unsafe fn avxpackblock19(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  19 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 19));
  w1 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 25));
  w0 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 31));
  w1 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 11));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 9));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 21));
  w0 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 13));
  _mm256_storeu_si256(compressed.offset(18), w0);
}


/* we are going to pack 256 20-bit values, touching 20 256-bit words, using 320 bytes */ 
unsafe fn avxpackblock20(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  20 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 12));
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 12));
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 12));
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 12));
  _mm256_storeu_si256(compressed.offset(19), w1);
}


/* we are going to pack 256 21-bit values, touching 21 256-bit words, using 336 bytes */ 
unsafe fn avxpackblock21(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  21 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(2)) , 10));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 9));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 19));
  w1 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(8)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 27));
  w0 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(20)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 25));
  w0 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 14));
  w1 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 13));
  w1 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(29)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 11));
  _mm256_storeu_si256(compressed.offset(20), w0);
}


/* we are going to pack 256 22-bit values, touching 22 256-bit words, using 352 bytes */ 
unsafe fn avxpackblock22(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  22 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 10));
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 14));
  w1 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 10));
  _mm256_storeu_si256(compressed.offset(21), w1);
}


/* we are going to pack 256 23-bit values, touching 23 256-bit words, using 368 bytes */ 
unsafe fn avxpackblock23(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  23 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 23));
  w1 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 19));
  w0 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 10));
  w1 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 11));
  w0 = _mm256_srli_epi32(tmp,21);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(14)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(17)) , 7));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 21));
  w0 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(24)) , 8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 13));
  w0 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(28)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 9));
  _mm256_storeu_si256(compressed.offset(22), w0);
}


/* we are going to pack 256 24-bit values, touching 24 256-bit words, using 384 bytes */ 
unsafe fn avxpackblock24(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  24 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(3)) , 8));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 8));
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(11)) , 8));
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(12));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 8));
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 8));
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(20));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 8));
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 8));
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(28));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 8));
  _mm256_storeu_si256(compressed.offset(23), w1);
}


/* we are going to pack 256 25-bit values, touching 25 256-bit words, using 400 bytes */ 
unsafe fn avxpackblock25(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  25 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 11));
  w1 = _mm256_srli_epi32(tmp,21);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(4)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 15));
  w0 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(9)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 19));
  w1 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(13)) , 5));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 9));
  w0 = _mm256_srli_epi32(tmp,23);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(18)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 13));
  w1 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(22)) , 6));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 17));
  w0 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 10));
  w1 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(27)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 7));
  _mm256_storeu_si256(compressed.offset(24), w0);
}


/* we are going to pack 256 26-bit values, touching 26 256-bit words, using 416 bytes */ 
unsafe fn avxpackblock26(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  26 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 14));
  w1 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(5)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 10));
  w0 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 6));
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 10));
  w1 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(26)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 6));
  _mm256_storeu_si256(compressed.offset(25), w1);
}


/* we are going to pack 256 27-bit values, touching 27 256-bit words, using 432 bytes */ 
unsafe fn avxpackblock27(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  27 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 7));
  w1 = _mm256_srli_epi32(tmp,25);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(6)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 29));
  w0 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 19));
  w0 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 14));
  w1 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 9));
  w0 = _mm256_srli_epi32(tmp,23);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(12)) , 4));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 31));
  w1 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 11));
  w1 = _mm256_srli_epi32(tmp,21);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 6));
  w0 = _mm256_srli_epi32(tmp,26);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(19)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 23));
  w0 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 13));
  w0 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(25)) , 3));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 10));
  w0 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 5));
  _mm256_storeu_si256(compressed.offset(26), w0);
}


/* we are going to pack 256 28-bit values, touching 28 256-bit words, using 448 bytes */ 
unsafe fn avxpackblock28(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  28 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(7)) , 4));
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(8));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 4));
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(23)) , 4));
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(24));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(26 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 4));
  _mm256_storeu_si256(compressed.offset(27), w1);
}


/* we are going to pack 256 29-bit values, touching 29 256-bit words, using 464 bytes */ 
unsafe fn avxpackblock29(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  29 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 23));
  w1 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 11));
  w1 = _mm256_srli_epi32(tmp,21);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 5));
  w1 = _mm256_srli_epi32(tmp,27);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(10)) , 2));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 31));
  w0 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 25));
  w0 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 19));
  w0 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 13));
  w0 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 10));
  w1 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 7));
  w0 = _mm256_srli_epi32(tmp,25);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 4));
  w1 = _mm256_srli_epi32(tmp,28);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(21)) , 1));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 9));
  w1 = _mm256_srli_epi32(tmp,23);
  _mm256_storeu_si256(compressed.offset(26 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 6));
  w0 = _mm256_srli_epi32(tmp,26);
  _mm256_storeu_si256(compressed.offset(27 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 3));
  _mm256_storeu_si256(compressed.offset(28), w0);
}


/* we are going to pack 256 30-bit values, touching 30 256-bit words, using 480 bytes */ 
unsafe fn avxpackblock30(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  30 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 30));
  w1 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 26));
  w1 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 22));
  w1 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 18));
  w1 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 14));
  w1 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 10));
  w1 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 6));
  w1 = _mm256_srli_epi32(tmp,26);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 4));
  w0 = _mm256_srli_epi32(tmp,28);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(15)) , 2));
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(16));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 28));
  w1 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 24));
  w1 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 20));
  w1 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 16));
  w1 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 12));
  w1 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 10));
  w0 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 8));
  w1 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(26 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 6));
  w0 = _mm256_srli_epi32(tmp,26);
  _mm256_storeu_si256(compressed.offset(27 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 4));
  w1 = _mm256_srli_epi32(tmp,28);
  _mm256_storeu_si256(compressed.offset(28 as isize), w0);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 2));
  _mm256_storeu_si256(compressed.offset(29), w1);
}


/* we are going to pack 256 31-bit values, touching 31 256-bit words, using 496 bytes */ 
unsafe fn avxpackblock31(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  31 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
// not a power of two.
let mut tmp: __m256i; /* used to store inputs at word boundary */
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  tmp = _mm256_lddqu_si256 (input_ptr.offset(1));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 31));
  w1 = _mm256_srli_epi32(tmp,1);
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(2));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 30));
  w0 = _mm256_srli_epi32(tmp,2);
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(3));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 29));
  w1 = _mm256_srli_epi32(tmp,3);
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(4));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 28));
  w0 = _mm256_srli_epi32(tmp,4);
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(5));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 27));
  w1 = _mm256_srli_epi32(tmp,5);
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(6));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 26));
  w0 = _mm256_srli_epi32(tmp,6);
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(7));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 25));
  w1 = _mm256_srli_epi32(tmp,7);
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(8));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 24));
  w0 = _mm256_srli_epi32(tmp,8);
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(9));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 23));
  w1 = _mm256_srli_epi32(tmp,9);
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(10));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 22));
  w0 = _mm256_srli_epi32(tmp,10);
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(11));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 21));
  w1 = _mm256_srli_epi32(tmp,11);
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(12));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 20));
  w0 = _mm256_srli_epi32(tmp,12);
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(13));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 19));
  w1 = _mm256_srli_epi32(tmp,13);
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(14));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 18));
  w0 = _mm256_srli_epi32(tmp,14);
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(15));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 17));
  w1 = _mm256_srli_epi32(tmp,15);
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(16));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 16));
  w0 = _mm256_srli_epi32(tmp,16);
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(17));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 15));
  w1 = _mm256_srli_epi32(tmp,17);
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(18));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 14));
  w0 = _mm256_srli_epi32(tmp,18);
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(19));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 13));
  w1 = _mm256_srli_epi32(tmp,19);
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(20));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 12));
  w0 = _mm256_srli_epi32(tmp,20);
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(21));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 11));
  w1 = _mm256_srli_epi32(tmp,21);
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(22));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 10));
  w0 = _mm256_srli_epi32(tmp,22);
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(23));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 9));
  w1 = _mm256_srli_epi32(tmp,23);
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(24));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 8));
  w0 = _mm256_srli_epi32(tmp,24);
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(25));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 7));
  w1 = _mm256_srli_epi32(tmp,25);
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(26));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 6));
  w0 = _mm256_srli_epi32(tmp,26);
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(27));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 5));
  w1 = _mm256_srli_epi32(tmp,27);
  _mm256_storeu_si256(compressed.offset(26 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(28));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 4));
  w0 = _mm256_srli_epi32(tmp,28);
  _mm256_storeu_si256(compressed.offset(27 as isize), w1);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(29));
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(tmp , 3));
  w1 = _mm256_srli_epi32(tmp,29);
  _mm256_storeu_si256(compressed.offset(28 as isize), w0);
  tmp = _mm256_lddqu_si256 (input_ptr.offset(30));
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(tmp , 2));
  w0 = _mm256_srli_epi32(tmp,30);
  _mm256_storeu_si256(compressed.offset(29 as isize), w1);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(_mm256_lddqu_si256(input_ptr.offset(31)) , 1));
  _mm256_storeu_si256(compressed.offset(30), w0);
}


/* we are going to pack 256 32-bit values, touching 32 256-bit words, using 512 bytes */ 
unsafe fn avxpackblock32(input_ptr: *const __m256i, compressed: *mut __m256i) {
  /* we are going to touch  32 256-bit words */ 
  let mut w0: __m256i;
  let mut w1: __m256i;
  w0 = _mm256_lddqu_si256 (input_ptr.offset(0));
  _mm256_storeu_si256(compressed.offset(0 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(1));
  _mm256_storeu_si256(compressed.offset(1 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(2));
  _mm256_storeu_si256(compressed.offset(2 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(3));
  _mm256_storeu_si256(compressed.offset(3 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(4));
  _mm256_storeu_si256(compressed.offset(4 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(5));
  _mm256_storeu_si256(compressed.offset(5 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(6));
  _mm256_storeu_si256(compressed.offset(6 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(7));
  _mm256_storeu_si256(compressed.offset(7 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(8));
  _mm256_storeu_si256(compressed.offset(8 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(9));
  _mm256_storeu_si256(compressed.offset(9 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(10));
  _mm256_storeu_si256(compressed.offset(10 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(11));
  _mm256_storeu_si256(compressed.offset(11 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(12));
  _mm256_storeu_si256(compressed.offset(12 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(13));
  _mm256_storeu_si256(compressed.offset(13 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(14));
  _mm256_storeu_si256(compressed.offset(14 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(15));
  _mm256_storeu_si256(compressed.offset(15 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(16));
  _mm256_storeu_si256(compressed.offset(16 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(17));
  _mm256_storeu_si256(compressed.offset(17 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(18));
  _mm256_storeu_si256(compressed.offset(18 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(19));
  _mm256_storeu_si256(compressed.offset(19 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(20));
  _mm256_storeu_si256(compressed.offset(20 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(21));
  _mm256_storeu_si256(compressed.offset(21 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(22));
  _mm256_storeu_si256(compressed.offset(22 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(23));
  _mm256_storeu_si256(compressed.offset(23 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(24));
  _mm256_storeu_si256(compressed.offset(24 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(25));
  _mm256_storeu_si256(compressed.offset(25 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(26));
  _mm256_storeu_si256(compressed.offset(26 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(27));
  _mm256_storeu_si256(compressed.offset(27 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(28));
  _mm256_storeu_si256(compressed.offset(28 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(29));
  _mm256_storeu_si256(compressed.offset(29 as isize), w1);
  w0 = _mm256_lddqu_si256 (input_ptr.offset(30));
  _mm256_storeu_si256(compressed.offset(30 as isize), w0);
  w1 = _mm256_lddqu_si256 (input_ptr.offset(31));
  _mm256_storeu_si256(compressed.offset(31), w1);
}


/* we packed 256 1-bit values, touching 1 256-bit words, using 16 bytes */ 
unsafe fn avxunpackblock1(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  1 256-bit word */ 
let mut w0: __m256i;
  let mask: __m256i = _mm256_set1_epi32(1);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 1) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 9) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 13) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 19) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 21) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 22) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 23) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 25) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 26) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 27) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 28) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 29) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 30) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 31) );
}


/* we packed 256 2-bit values, touching 2 256-bit words, using 32 bytes */ 
unsafe fn avxunpackblock2(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  2 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(3);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 22) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 26) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 28) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 30) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 18) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 22) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 26) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 28) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 30) );
}


/* we packed 256 3-bit values, touching 3 256-bit words, using 48 bytes */ 
unsafe fn avxunpackblock3(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  3 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(7);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 9) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 21) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 27) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 7) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 13) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 19) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 22) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 25) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 28) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 23) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 26) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 29) );
}


/* we packed 256 4-bit values, touching 4 256-bit words, using 64 bytes */ 
unsafe fn avxunpackblock4(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  4 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(15);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w0, 28) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 28) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w0, 28) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 28) );
}


/* we packed 256 5-bit values, touching 5 256-bit words, using 80 bytes */ 
unsafe fn avxunpackblock5(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  5 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(31);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 25) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 13) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 18) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 23) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 1) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 21) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 26) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 31) ,_mm256_slli_epi32(w1, 1) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 19) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 22) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 27) );
}


/* we packed 256 6-bit values, touching 6 256-bit words, using 96 bytes */ 
unsafe fn avxunpackblock6(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  6 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(63);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 24) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 22) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 26) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 18) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 22) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 26) );
}


/* we packed 256 7-bit values, touching 7 256-bit words, using 112 bytes */ 
unsafe fn avxunpackblock7(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  7 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(127);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 21) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 17) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 24) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 13) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 23) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 19) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 15) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 22) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 25) );
}


/* we packed 256 8-bit values, touching 8 256-bit words, using 128 bytes */ 
unsafe fn avxunpackblock8(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  8 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(255);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(3), _mm256_srli_epi32(w0, 24) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w1, 24) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(11), _mm256_srli_epi32(w0, 24) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 24) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(19), _mm256_srli_epi32(w0, 24) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w1, 24) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  _mm256_storeu_si256(out.offset(27), _mm256_srli_epi32(w0, 24) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 24) );
}


/* we packed 256 9-bit values, touching 9 256-bit words, using 144 bytes */ 
unsafe fn avxunpackblock9(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  9 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(511);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 9) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 13) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 22) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 21) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 11) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 19) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 23) );
}


/* we packed 256 10-bit values, touching 10 256-bit words, using 160 bytes */ 
unsafe fn avxunpackblock10(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  10 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(1023);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 20) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 18) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 22) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 22) );
}


/* we packed 256 11-bit values, touching 11 256-bit words, using 176 bytes */ 
unsafe fn avxunpackblock11(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  11 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(2047);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 13) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 25) ,_mm256_slli_epi32(w0, 7) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 5) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 27) ,_mm256_slli_epi32(w0, 5) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 7) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 18) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 19) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 20) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 21) );
}


/* we packed 256 12-bit values, touching 12 256-bit words, using 192 bytes */ 
unsafe fn avxunpackblock12(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  12 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(4095);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w0, 20) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 20) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w0, 20) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 20) );
}


/* we packed 256 13-bit values, touching 13 256-bit words, using 208 bytes */ 
unsafe fn avxunpackblock13(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  13 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(8191);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 13) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 7) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 1) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 21) ,_mm256_slli_epi32(w0, 11) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 17) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 11) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 18) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 31) ,_mm256_slli_epi32(w1, 1) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 25) ,_mm256_slli_epi32(w0, 7) ) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 19) );
}


/* we packed 256 14-bit values, touching 14 256-bit words, using 224 bytes */ 
unsafe fn avxunpackblock14(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  14 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(16383);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 18) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 16) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 18) );
}


/* we packed 256 15-bit values, touching 15 256-bit words, using 240 bytes */ 
unsafe fn avxunpackblock15(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  15 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(32767);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 15) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 13) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 5) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  _mm256_storeu_si256(out.offset(15),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 16) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 14) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 27) ,_mm256_slli_epi32(w0, 5) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 19) ,_mm256_slli_epi32(w0, 13) ) ) );
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 17) );
}


/* we packed 256 16-bit values, touching 16 256-bit words, using 256 bytes */ 
unsafe fn avxunpackblock16(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  16 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(65535);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(1), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(3), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(5), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(9), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(11), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(13), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(17), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(19), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(21), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(25), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(27), _mm256_srli_epi32(w1, 16) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask,  w0 ) );
  _mm256_storeu_si256(out.offset(29), _mm256_srli_epi32(w0, 16) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(30),  _mm256_and_si256( mask,  w1 ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 16) );
}


/* we packed 256 17-bit values, touching 17 256-bit words, using 272 bytes */ 
unsafe fn avxunpackblock17(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  17 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(131071);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 19) ,_mm256_slli_epi32(w0, 13) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 27) ,_mm256_slli_epi32(w0, 5) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 14) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 5) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 11) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 13) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 15) );
}


/* we packed 256 18-bit values, touching 18 256-bit words, using 288 bytes */ 
unsafe fn avxunpackblock18(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  18 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(262143);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 12) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 14) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 14) );
}


/* we packed 256 19-bit values, touching 19 256-bit words, using 304 bytes */ 
unsafe fn avxunpackblock19(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  19 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(524287);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 19) ,_mm256_slli_epi32(w1, 13) ) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 25) ,_mm256_slli_epi32(w0, 7) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 12) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 31) ,_mm256_slli_epi32(w1, 1) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 11) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 10) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 3) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 21) ,_mm256_slli_epi32(w0, 11) ) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 1) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 7) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 13) );
}


/* we packed 256 20-bit values, touching 20 256-bit words, using 320 bytes */ 
unsafe fn avxunpackblock20(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  20 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(1048575);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w0, 12) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 12) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w0, 12) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 12) );
}


/* we packed 256 21-bit values, touching 21 256-bit words, using 336 bytes */ 
unsafe fn avxunpackblock21(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  21 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(2097151);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  _mm256_storeu_si256(out.offset(2),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 10) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 9) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 19) ,_mm256_slli_epi32(w1, 13) ) ) );
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  _mm256_storeu_si256(out.offset(11),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 7) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 27) ,_mm256_slli_epi32(w0, 5) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 5) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 25) ,_mm256_slli_epi32(w0, 7) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 14) ,_mm256_slli_epi32(w1, 18) ) ) );
  _mm256_storeu_si256(out.offset(23),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 13) ,_mm256_slli_epi32(w1, 19) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  _mm256_storeu_si256(out.offset(29),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 11) );
}


/* we packed 256 22-bit values, touching 22 256-bit words, using 352 bytes */ 
unsafe fn avxunpackblock22(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  22 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(4194303);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 6) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 8) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 10) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 14) ,_mm256_slli_epi32(w1, 18) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 10) );
}


/* we packed 256 23-bit values, touching 23 256-bit words, using 368 bytes */ 
unsafe fn avxunpackblock23(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  23 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(8388607);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 23) ,_mm256_slli_epi32(w1, 9) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  _mm256_storeu_si256(out.offset(3),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 19) ,_mm256_slli_epi32(w0, 13) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 10) ,_mm256_slli_epi32(w1, 22) ) ) );
  _mm256_storeu_si256(out.offset(7),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 11) ,_mm256_slli_epi32(w0, 21) ) ) );
  _mm256_storeu_si256(out.offset(14),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(17),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 7) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 21) ,_mm256_slli_epi32(w0, 11) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 8) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 13) ,_mm256_slli_epi32(w0, 19) ) ) );
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 9) );
}


/* we packed 256 24-bit values, touching 24 256-bit words, using 384 bytes */ 
unsafe fn avxunpackblock24(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  24 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(16777215);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(3), _mm256_srli_epi32(w0, 8) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w1, 8) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(11), _mm256_srli_epi32(w0, 8) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 8) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(19), _mm256_srli_epi32(w0, 8) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(20),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w1, 8) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  _mm256_storeu_si256(out.offset(27), _mm256_srli_epi32(w0, 8) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(28),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 8) );
}


/* we packed 256 25-bit values, touching 25 256-bit words, using 400 bytes */ 
unsafe fn avxunpackblock25(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  25 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(33554431);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 11) ,_mm256_slli_epi32(w1, 21) ) ) );
  _mm256_storeu_si256(out.offset(4),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 15) ,_mm256_slli_epi32(w0, 17) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  _mm256_storeu_si256(out.offset(9),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 19) ,_mm256_slli_epi32(w1, 13) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  _mm256_storeu_si256(out.offset(13),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 5) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 9) ,_mm256_slli_epi32(w0, 23) ) ) );
  _mm256_storeu_si256(out.offset(18),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 13) ,_mm256_slli_epi32(w1, 19) ) ) );
  _mm256_storeu_si256(out.offset(22),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 6) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 17) ,_mm256_slli_epi32(w0, 15) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 10) ,_mm256_slli_epi32(w1, 22) ) ) );
  _mm256_storeu_si256(out.offset(27),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 7) );
}


/* we packed 256 26-bit values, touching 26 256-bit words, using 416 bytes */ 
unsafe fn avxunpackblock26(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  26 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(67108863);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 14) ,_mm256_slli_epi32(w1, 18) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  _mm256_storeu_si256(out.offset(5),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 2) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 10) ,_mm256_slli_epi32(w0, 22) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 6) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 10) ,_mm256_slli_epi32(w1, 22) ) ) );
  _mm256_storeu_si256(out.offset(26),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 4) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 6) );
}


/* we packed 256 27-bit values, touching 27 256-bit words, using 432 bytes */ 
unsafe fn avxunpackblock27(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  27 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(134217727);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 7) ,_mm256_slli_epi32(w1, 25) ) ) );
  _mm256_storeu_si256(out.offset(6),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 29) ,_mm256_slli_epi32(w0, 3) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 19) ,_mm256_slli_epi32(w0, 13) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 14) ,_mm256_slli_epi32(w1, 18) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 9) ,_mm256_slli_epi32(w0, 23) ) ) );
  _mm256_storeu_si256(out.offset(12),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 4) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 31) ,_mm256_slli_epi32(w1, 1) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 11) ,_mm256_slli_epi32(w1, 21) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 6) ,_mm256_slli_epi32(w0, 26) ) ) );
  _mm256_storeu_si256(out.offset(19),  _mm256_and_si256( mask, _mm256_srli_epi32(w0, 1) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 23) ,_mm256_slli_epi32(w0, 9) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 13) ,_mm256_slli_epi32(w0, 19) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  _mm256_storeu_si256(out.offset(25),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 3) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 10) ,_mm256_slli_epi32(w0, 22) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 5) );
}


/* we packed 256 28-bit values, touching 28 256-bit words, using 448 bytes */ 
unsafe fn avxunpackblock28(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  28 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(268435455);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  _mm256_storeu_si256(out.offset(7), _mm256_srli_epi32(w0, 4) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(8),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w1, 4) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  _mm256_storeu_si256(out.offset(23), _mm256_srli_epi32(w0, 4) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(24),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(27));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 4) );
}


/* we packed 256 29-bit values, touching 29 256-bit words, using 464 bytes */ 
unsafe fn avxunpackblock29(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  29 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(536870911);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 23) ,_mm256_slli_epi32(w1, 9) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 11) ,_mm256_slli_epi32(w1, 21) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 5) ,_mm256_slli_epi32(w1, 27) ) ) );
  _mm256_storeu_si256(out.offset(10),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 2) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 31) ,_mm256_slli_epi32(w0, 1) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 25) ,_mm256_slli_epi32(w0, 7) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 19) ,_mm256_slli_epi32(w0, 13) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 13) ,_mm256_slli_epi32(w0, 19) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 10) ,_mm256_slli_epi32(w1, 22) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 7) ,_mm256_slli_epi32(w0, 25) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 4) ,_mm256_slli_epi32(w1, 28) ) ) );
  _mm256_storeu_si256(out.offset(21),  _mm256_and_si256( mask, _mm256_srli_epi32(w1, 1) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(27));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 9) ,_mm256_slli_epi32(w1, 23) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(28));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 6) ,_mm256_slli_epi32(w0, 26) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 3) );
}


/* we packed 256 30-bit values, touching 30 256-bit words, using 480 bytes */ 
unsafe fn avxunpackblock30(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  30 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(1073741823);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 30) ,_mm256_slli_epi32(w1, 2) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 26) ,_mm256_slli_epi32(w1, 6) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 22) ,_mm256_slli_epi32(w1, 10) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 18) ,_mm256_slli_epi32(w1, 14) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 14) ,_mm256_slli_epi32(w1, 18) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 10) ,_mm256_slli_epi32(w1, 22) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 6) ,_mm256_slli_epi32(w1, 26) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 4) ,_mm256_slli_epi32(w0, 28) ) ) );
  _mm256_storeu_si256(out.offset(15), _mm256_srli_epi32(w0, 2) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(16),  _mm256_and_si256( mask,  w1 ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 28) ,_mm256_slli_epi32(w1, 4) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 24) ,_mm256_slli_epi32(w1, 8) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 20) ,_mm256_slli_epi32(w1, 12) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 16) ,_mm256_slli_epi32(w1, 16) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 12) ,_mm256_slli_epi32(w1, 20) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 10) ,_mm256_slli_epi32(w0, 22) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(27));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 8) ,_mm256_slli_epi32(w1, 24) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(28));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 6) ,_mm256_slli_epi32(w0, 26) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(29));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 4) ,_mm256_slli_epi32(w1, 28) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w1, 2) );
}


/* we packed 256 31-bit values, touching 31 256-bit words, using 496 bytes */ 
unsafe fn avxunpackblock31(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  31 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  let mask: __m256i = _mm256_set1_epi32(2147483647);
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  _mm256_and_si256( mask,  w0 ) );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 31) ,_mm256_slli_epi32(w1, 1) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 30) ,_mm256_slli_epi32(w0, 2) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 29) ,_mm256_slli_epi32(w1, 3) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 28) ,_mm256_slli_epi32(w0, 4) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 27) ,_mm256_slli_epi32(w1, 5) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(6),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 26) ,_mm256_slli_epi32(w0, 6) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(7),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 25) ,_mm256_slli_epi32(w1, 7) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(8),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 24) ,_mm256_slli_epi32(w0, 8) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(9),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 23) ,_mm256_slli_epi32(w1, 9) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(10),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 22) ,_mm256_slli_epi32(w0, 10) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(11),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 21) ,_mm256_slli_epi32(w1, 11) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(12),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 20) ,_mm256_slli_epi32(w0, 12) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(13),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 19) ,_mm256_slli_epi32(w1, 13) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(14),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 18) ,_mm256_slli_epi32(w0, 14) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(15),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 17) ,_mm256_slli_epi32(w1, 15) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(16),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 16) ,_mm256_slli_epi32(w0, 16) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(17),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 15) ,_mm256_slli_epi32(w1, 17) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(18),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 14) ,_mm256_slli_epi32(w0, 18) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(19),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 13) ,_mm256_slli_epi32(w1, 19) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(20),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 12) ,_mm256_slli_epi32(w0, 20) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(21),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 11) ,_mm256_slli_epi32(w1, 21) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(22),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 10) ,_mm256_slli_epi32(w0, 22) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(23),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 9) ,_mm256_slli_epi32(w1, 23) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(24),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 8) ,_mm256_slli_epi32(w0, 24) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(25),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 7) ,_mm256_slli_epi32(w1, 25) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(26),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 6) ,_mm256_slli_epi32(w0, 26) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(27));
  _mm256_storeu_si256(out.offset(27),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 5) ,_mm256_slli_epi32(w1, 27) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(28));
  _mm256_storeu_si256(out.offset(28),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 4) ,_mm256_slli_epi32(w0, 28) ) ) );
  w1 = _mm256_lddqu_si256(compressed.offset(29));
  _mm256_storeu_si256(out.offset(29),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w0, 3) ,_mm256_slli_epi32(w1, 29) ) ) );
  w0 = _mm256_lddqu_si256(compressed.offset(30));
  _mm256_storeu_si256(out.offset(30),     _mm256_and_si256( mask,  _mm256_or_si256(_mm256_srli_epi32(w1, 2) ,_mm256_slli_epi32(w0, 30) ) ) );
  _mm256_storeu_si256(out.offset(31), _mm256_srli_epi32(w0, 1) );
}


/* we packed 256 32-bit values, touching 32 256-bit words, using 512 bytes */ 
unsafe fn avxunpackblock32(compressed: *const __m256i, out: *mut __m256i) {
  /* we are going to access  32 256-bit words */ 
let mut w0: __m256i;
let mut w1: __m256i;
  w0 = _mm256_lddqu_si256(compressed);
  _mm256_storeu_si256(out.offset(0),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(1));
  _mm256_storeu_si256(out.offset(1),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(2));
  _mm256_storeu_si256(out.offset(2),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(3));
  _mm256_storeu_si256(out.offset(3),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(4));
  _mm256_storeu_si256(out.offset(4),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(5));
  _mm256_storeu_si256(out.offset(5),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(6));
  _mm256_storeu_si256(out.offset(6),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(7));
  _mm256_storeu_si256(out.offset(7),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(8));
  _mm256_storeu_si256(out.offset(8),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(9));
  _mm256_storeu_si256(out.offset(9),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(10));
  _mm256_storeu_si256(out.offset(10),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(11));
  _mm256_storeu_si256(out.offset(11),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(12));
  _mm256_storeu_si256(out.offset(12),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(13));
  _mm256_storeu_si256(out.offset(13),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(14));
  _mm256_storeu_si256(out.offset(14),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(15));
  _mm256_storeu_si256(out.offset(15),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(16));
  _mm256_storeu_si256(out.offset(16),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(17));
  _mm256_storeu_si256(out.offset(17),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(18));
  _mm256_storeu_si256(out.offset(18),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(19));
  _mm256_storeu_si256(out.offset(19),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(20));
  _mm256_storeu_si256(out.offset(20),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(21));
  _mm256_storeu_si256(out.offset(21),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(22));
  _mm256_storeu_si256(out.offset(22),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(23));
  _mm256_storeu_si256(out.offset(23),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(24));
  _mm256_storeu_si256(out.offset(24),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(25));
  _mm256_storeu_si256(out.offset(25),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(26));
  _mm256_storeu_si256(out.offset(26),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(27));
  _mm256_storeu_si256(out.offset(27),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(28));
  _mm256_storeu_si256(out.offset(28),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(29));
  _mm256_storeu_si256(out.offset(29),  w1 );
  w0 = _mm256_lddqu_si256(compressed.offset(30));
  _mm256_storeu_si256(out.offset(30),  w0 );
  w1 = _mm256_lddqu_si256(compressed.offset(31));
  _mm256_storeu_si256(out.offset(31),  w1 );
}


unsafe fn avxunpackblock(compressed: *const __m256i, out: *mut __m256i, num_bits: u8) {
    match num_bits {
		0 => avxunpackblock0(compressed, out),
		1 => avxunpackblock1(compressed, out),
		2 => avxunpackblock2(compressed, out),
		3 => avxunpackblock3(compressed, out),
		4 => avxunpackblock4(compressed, out),
		5 => avxunpackblock5(compressed, out),
		6 => avxunpackblock6(compressed, out),
		7 => avxunpackblock7(compressed, out),
		8 => avxunpackblock8(compressed, out),
		9 => avxunpackblock9(compressed, out),
		10 => avxunpackblock10(compressed, out),
		11 => avxunpackblock11(compressed, out),
		12 => avxunpackblock12(compressed, out),
		13 => avxunpackblock13(compressed, out),
		14 => avxunpackblock14(compressed, out),
		15 => avxunpackblock15(compressed, out),
		16 => avxunpackblock16(compressed, out),
		17 => avxunpackblock17(compressed, out),
		18 => avxunpackblock18(compressed, out),
		19 => avxunpackblock19(compressed, out),
		20 => avxunpackblock20(compressed, out),
		21 => avxunpackblock21(compressed, out),
		22 => avxunpackblock22(compressed, out),
		23 => avxunpackblock23(compressed, out),
		24 => avxunpackblock24(compressed, out),
		25 => avxunpackblock25(compressed, out),
		26 => avxunpackblock26(compressed, out),
		27 => avxunpackblock27(compressed, out),
		28 => avxunpackblock28(compressed, out),
		29 => avxunpackblock29(compressed, out),
		30 => avxunpackblock30(compressed, out),
		31 => avxunpackblock31(compressed, out),
		32 => avxunpackblock32(compressed, out),
		_ => panic!("num_bits must be between [0..num_bits].")
	}
}
